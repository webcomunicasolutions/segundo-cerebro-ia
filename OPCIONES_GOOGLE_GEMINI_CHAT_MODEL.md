# Google Gemini Chat Model - Documentaci√≥n Completa

## üìã Informaci√≥n General
- **Tipo**: `nodes-langchain.lmChatGoogleGemini`
- **Versi√≥n**: 1
- **Categor√≠a**: Transform
- **Package**: `@n8n/n8n-nodes-langchain`
- **Descripci√≥n**: Chat Model Google Gemini - Nodo para usar modelos de chat Gemini de Google con agentes conversacionales

## üéØ Prop√≥sito del Nodo

Este nodo proporciona acceso a los modelos de lenguaje de chat de Google Gemini (incluyendo Gemini 2.5 Flash) dentro de workflows de n8n que utilizan la arquitectura LangChain. Es un **nodo de modelo de lenguaje** que debe conectarse a una cadena de IA (AI Chain) o agente de IA (AI Agent).

## üîå Tipo de Conexi√≥n

- **Debe conectarse a**: AI Chain, AI Agent, o componentes LangChain
- **Output**: Model (se conecta a nodos downstream que consumen el modelo)
- **Uso t√≠pico**: Motor de LLM para AI Agents conversacionales

## üîë Credenciales Requeridas

- **Credential**: `googlePalmApi` (Google AI API Key)
- **Requerida**: S√≠
- **Documentaci√≥n**: [Credenciales Google AI](https://docs.n8n.io/integrations/builtin/credentials/googleai/)

## üìù Par√°metros Principales

### 1. Model (`modelName`)
- **Tipo**: `options` (desplegable din√°mico)
- **Default**: `models/gemini-2.5-flash`
- **Descripci√≥n**: El modelo que generar√° la respuesta
- **Valores posibles**: Se cargan din√°micamente desde la API de Google Gemini
  - `models/gemini-2.5-flash` (recomendado para velocidad y eficiencia)
  - `models/gemini-2.5-pro` (para tareas m√°s complejas)
  - Otros modelos disponibles seg√∫n tu cuenta de Google AI
- **Documentaci√≥n**: [Lista de modelos Gemini](https://developers.generativeai.google/api/rest/generativelanguage/models/list)

**üí° Nota**: n8n carga autom√°ticamente solo los modelos disponibles para tu cuenta, excluyendo modelos de embeddings.

## ‚öôÔ∏è Opciones Disponibles (Campo `options`)

### 1. Maximum Number of Tokens (`maxOutputTokens`)
- **Tipo**: `number`
- **Default**: `2048`
- **Descripci√≥n**: El n√∫mero m√°ximo de tokens que se generar√°n en la respuesta
- **Rango recomendado**: 256 - 8192 (dependiendo del modelo)
- **Prop√≥sito**: Controla la longitud m√°xima de la respuesta generada

**Cu√°ndo ajustar**:
- **Valores bajos (256-512)**: Respuestas cortas y concisas (confirmaciones, clasificaciones)
- **Valores medios (1024-2048)**: Respuestas est√°ndar (conversaci√≥n, an√°lisis moderado)
- **Valores altos (4096-8192)**: Respuestas detalladas (documentaci√≥n extensa, an√°lisis profundo)

### 2. Sampling Temperature (`temperature`)
- **Tipo**: `number`
- **Default**: `0.4`
- **Rango**: 0.0 - 1.0
- **Precisi√≥n**: 1 decimal
- **Descripci√≥n**: Controla la aleatoriedad en la generaci√≥n de respuestas

**Comportamiento**:
- **0.0 - 0.2**: Determin√≠stico, repetible, preciso (ideal para tareas que requieren consistencia)
- **0.3 - 0.5**: Balance entre creatividad y consistencia (recomendado para agentes)
- **0.6 - 1.0**: M√°s creativo, diverso, pero mayor riesgo de alucinaciones

**Cu√°ndo usar**:
- **Temperature baja (0.1-0.2)**: Extracci√≥n de datos, clasificaci√≥n, an√°lisis estructurado
- **Temperature media (0.4-0.5)**: Conversaci√≥n natural, asistentes virtuales
- **Temperature alta (0.7-1.0)**: Generaci√≥n creativa, brainstorming

### 3. Top K (`topK`)
- **Tipo**: `number`
- **Default**: `32`
- **Rango**: -1 - 40
- **Descripci√≥n**: N√∫mero de opciones de tokens que el modelo considera para generar el siguiente token

**Comportamiento**:
- **-1**: Deshabilitado (sin l√≠mite de opciones)
- **1-10**: Muy restrictivo (respuestas m√°s predecibles)
- **20-40**: Moderadamente restrictivo (balance entre diversidad y calidad)

**Prop√≥sito**: Elimina opciones de baja probabilidad ("long tail") para mejorar calidad de respuestas.

### 4. Top P (`topP`)
- **Tipo**: `number`
- **Default**: `1.0`
- **Rango**: 0.0 - 1.0
- **Precisi√≥n**: 1 decimal
- **Descripci√≥n**: Controla diversidad mediante nucleus sampling (muestreo de n√∫cleo)

**Comportamiento**:
- **0.5**: Solo se consideran las opciones que suman el 50% de probabilidad
- **0.9**: Se consideran opciones que suman el 90% de probabilidad
- **1.0**: Se consideran todas las opciones

**‚ö†Ô∏è Importante**: Google recomienda ajustar **o bien temperature o bien topP**, no ambos simult√°neamente, ya que interact√∫an entre s√≠.

### 5. Safety Settings (`safetySettings`)
- **Tipo**: `fixedCollection` (colecci√≥n de m√∫ltiples valores)
- **M√∫ltiples valores**: S√≠ (puedes definir m√∫ltiples categor√≠as de seguridad)
- **Default**:
  ```json
  {
    "category": "HARM_CATEGORY_HARASSMENT",
    "threshold": "HARM_BLOCK_THRESHOLD_UNSPECIFIED"
  }
  ```

#### Categor√≠as de Seguridad (`category`)

| Valor | Descripci√≥n |
|-------|-------------|
| `HARM_CATEGORY_HARASSMENT` | Contenido de acoso |
| `HARM_CATEGORY_HATE_SPEECH` | Discurso de odio y contenido ofensivo |
| `HARM_CATEGORY_SEXUALLY_EXPLICIT` | Contenido sexualmente expl√≠cito |
| `HARM_CATEGORY_DANGEROUS_CONTENT` | Contenido peligroso (violencia, autolesiones, etc.) |

#### Umbrales de Bloqueo (`threshold`)

| Valor | Descripci√≥n |
|-------|-------------|
| `HARM_BLOCK_THRESHOLD_UNSPECIFIED` | Sin especificar (usa configuraci√≥n por defecto de Google) |
| `BLOCK_LOW_AND_ABOVE` | Bloquea TODO excepto contenido NEGLIGIBLE |
| `BLOCK_MEDIUM_AND_ABOVE` | Permite contenido NEGLIGIBLE y LOW |
| `BLOCK_ONLY_HIGH` | Permite contenido NEGLIGIBLE, LOW y MEDIUM |
| `BLOCK_NONE` | Permite TODO el contenido (sin filtros) |

**Documentaci√≥n oficial**: [Gemini API Safety Settings](https://ai.google.dev/docs/safety_setting_gemini)

## üéØ Configuraci√≥n Recomendada

### Para AI Agent de Segundo Cerebro (Caso Actual)

```json
{
  "modelName": "models/gemini-2.5-flash",
  "options": {
    "temperature": 0.1,
    "maxOutputTokens": 2048,
    "topK": 32,
    "topP": 1.0,
    "safetySettings": {
      "values": [
        {
          "category": "HARM_CATEGORY_HARASSMENT",
          "threshold": "BLOCK_NONE"
        },
        {
          "category": "HARM_CATEGORY_HATE_SPEECH",
          "threshold": "BLOCK_NONE"
        },
        {
          "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
          "threshold": "BLOCK_NONE"
        },
        {
          "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
          "threshold": "BLOCK_NONE"
        }
      ]
    }
  }
}
```

**Raz√≥n de esta configuraci√≥n**:

1. **`temperature: 0.1`** (muy baja):
   - El agente debe ser **determin√≠stico y consistente** al clasificar notas
   - Necesitamos que siempre estructure datos de la misma forma (JSON Schema Enforcement)
   - Evita creatividad no deseada en tareas de extracci√≥n de datos
   - **Ejemplo**: "Comprar leche" siempre debe identificarse como Tarea, no como Nota o Recurso

2. **`maxOutputTokens: 2048`**:
   - Suficiente para respuestas estructuradas JSON
   - Balance entre capacidad de respuesta y costos
   - Permite respuestas completas sin cortar metadatos importantes

3. **`topK: 32`** (default):
   - Balance adecuado para respuestas estructuradas
   - No es necesario ajustar para este caso de uso

4. **`topP: 1.0`** (sin restricci√≥n):
   - Dejamos que temperature controle la aleatoriedad
   - Seguimos la recomendaci√≥n de Google de no ajustar ambos par√°metros

5. **`safetySettings: BLOCK_NONE`** (todos los filtros desactivados):
   - El sistema procesa **notas personales del usuario**
   - No queremos que el filtro de seguridad bloquee contenido leg√≠timo
   - **Ejemplo**: Si el usuario escribe "Recordar revisar documentaci√≥n sobre c√≥mo manejar contenido peligroso en la app", no queremos que se bloquee por `HARM_CATEGORY_DANGEROUS_CONTENT`
   - Es un contexto privado y personal, no contenido p√∫blico

### Para Asistente Conversacional Creativo

```json
{
  "modelName": "models/gemini-2.5-flash",
  "options": {
    "temperature": 0.7,
    "maxOutputTokens": 4096,
    "topK": 40,
    "topP": 0.95,
    "safetySettings": {
      "values": [
        {
          "category": "HARM_CATEGORY_HARASSMENT",
          "threshold": "BLOCK_MEDIUM_AND_ABOVE"
        },
        {
          "category": "HARM_CATEGORY_HATE_SPEECH",
          "threshold": "BLOCK_MEDIUM_AND_ABOVE"
        }
      ]
    }
  }
}
```

**Raz√≥n**: Mayor creatividad para conversaci√≥n natural, respuestas m√°s largas, filtros moderados de seguridad.

### Para An√°lisis de Documentos con M√°xima Precisi√≥n

```json
{
  "modelName": "models/gemini-2.5-pro",
  "options": {
    "temperature": 0.0,
    "maxOutputTokens": 8192,
    "topK": 10,
    "topP": 1.0
  }
}
```

**Raz√≥n**: Temperature 0.0 para m√°xima determinismo, modelo Pro para razonamiento complejo, tokens altos para an√°lisis exhaustivos.

## üí° Casos de Uso Comunes

### Caso 1: Clasificaci√≥n y Extracci√≥n de Datos (PKM/Segundo Cerebro)

**Configuraci√≥n**:
- Model: `gemini-2.5-flash`
- Temperature: `0.1` (muy baja)
- Max Tokens: `2048`
- Safety: `BLOCK_NONE`

**Uso**:
- Conectar a AI Agent con System Prompt que define JSON Schema
- El agente analiza mensajes del usuario y extrae metadatos estructurados
- Respuestas consistentes y predecibles

**Ejemplo de Prompt**:
```
Analiza el mensaje del usuario y clasif√≠calo seg√∫n estas categor√≠as:
- Nota: Idea, reflexi√≥n, apunte
- Tarea: Acci√≥n con verbo ejecutable
- Recurso: Referencia externa (link, libro, video)

Devuelve JSON con:
{
  "tipo": "Nota|Tarea|Recurso",
  "titulo": "...",
  "descripcion": "...",
  "prioridad": 1-5,
  "metadatos": {...}
}
```

### Caso 2: Asistente Virtual Conversacional

**Configuraci√≥n**:
- Model: `gemini-2.5-flash`
- Temperature: `0.5` (media)
- Max Tokens: `4096`
- Safety: `BLOCK_MEDIUM_AND_ABOVE`

**Uso**:
- Chatbot que responde preguntas de usuarios
- Conversaci√≥n natural con personalidad
- Respuestas variadas pero controladas

### Caso 3: Generaci√≥n de Contenido Creativo

**Configuraci√≥n**:
- Model: `gemini-2.5-pro`
- Temperature: `0.8` (alta)
- Max Tokens: `8192`
- Top K: `40`

**Uso**:
- Generaci√≥n de copy publicitario
- Escritura creativa (historias, art√≠culos)
- Brainstorming de ideas

## üîÑ Integraci√≥n con AI Agent

Este nodo **debe conectarse obligatoriamente** a un nodo de tipo:
- **AI Agent** (recomendado para workflows interactivos)
- **AI Chain** (para flujos de procesamiento secuencial)
- Otros nodos LangChain que consumen modelos de chat

**Flujo t√≠pico en n8n**:
```
Telegram Trigger ‚Üí AI Agent ‚Üí Google Gemini Chat Model ‚Üí [Tools/Memoria/Output]
                      ‚Üì
                 System Prompt
                      ‚Üì
                 User Message
```

**‚ö†Ô∏è Error com√∫n**: Si intentas usar este nodo sin conectarlo a una cadena de IA, ver√°s el mensaje:
> "This node must be connected to an AI chain"

## üÜö Google Gemini Chat Model vs Google Gemini (nodo est√°ndar)

| Aspecto | Google Gemini Chat Model | Google Gemini |
|---------|-------------------------|---------------|
| **Arquitectura** | LangChain (chat conversacional) | Nodo est√°ndar de n8n |
| **Uso** | AI Agents, AI Chains | Workflows normales de n8n |
| **Contexto** | Mantiene conversaci√≥n multi-turno | Una llamada, una respuesta |
| **Integraci√≥n** | Se conecta a componentes LangChain | Se conecta a nodos n8n normales |
| **Recomendado para** | Chatbots, agentes conversacionales | Procesamiento de texto puntual |

## ‚ö†Ô∏è Consideraciones Importantes

### 1. Latencia y Velocidad
- **Gemini 2.5 Flash**: Optimizado para latencia < 1 segundo
- **Gemini 2.5 Pro**: Mayor latencia pero mejor razonamiento complejo
- **Recomendaci√≥n**: Usa Flash para workflows en tiempo real (Telegram, chat), usa Pro para an√°lisis offline

### 2. Costos
- Los tokens se cobran seg√∫n el modelo usado
- `maxOutputTokens` limita costos al restringir longitud de respuesta
- Gemini Flash es m√°s econ√≥mico que Gemini Pro
- **Tip**: Monitorea uso de tokens en workflows intensivos

### 3. JSON Schema Enforcement
- Gemini 2.5 Flash tiene soporte **nativo** para `response_schema` (JSON Schema)
- No necesitas validaci√≥n post-generaci√≥n - el modelo garantiza salida estructurada
- **Ventaja competitiva**: M√°s r√°pido y confiable que Claude/GPT con JSON parsing manual

### 4. Capacidades Multimodales
- Gemini soporta **texto, audio e im√°genes** nativamente
- Puedes enviar screenshots, fotos, grabaciones de voz
- √ötil para workflows con Telegram que reciben media

### 5. Safety Settings en Contextos Privados
- Si el agente procesa notas personales, considera usar `BLOCK_NONE`
- Los filtros de seguridad pueden bloquear contenido leg√≠timo del usuario
- **Ejemplo**: Notas sobre temas sensibles pero v√°lidos (salud, finanzas, seguridad inform√°tica)

### 6. Temperature vs Top P
- Google recomienda ajustar **solo uno** de estos par√°metros
- Generalmente, ajusta `temperature` y deja `topP` en 1.0
- Si necesitas control m√°s fino, usa `topP` con temperature fija en 1.0

### 7. Modelos Disponibles Din√°micamente
- n8n carga modelos desde tu cuenta de Google AI en tiempo real
- Solo ver√°s modelos disponibles para tu API key
- Los modelos de embeddings se filtran autom√°ticamente

## üìö Referencias

- **Documentaci√≥n oficial n8n**: [Google Gemini Chat Model](https://docs.n8n.io/integrations/builtin/cluster-nodes/root-nodes/n8n-nodes-langchain.lmchatgooglegemini/)
- **LangChain Documentation**: [Google Gemini Integration](https://js.langchain.com/docs/integrations/chat/google_generativeai)
- **Google AI Documentation**: [Gemini API](https://ai.google.dev/docs)
- **Gemini Safety Settings**: [Safety Configuration](https://ai.google.dev/docs/safety_setting_gemini)
- **Lista de modelos**: [Generative Language API Models](https://developers.generativeai.google/api/rest/generativelanguage/models/list)

## üéì Contexto del Proyecto "Segundo Cerebro"

En el workflow "segundo_cerebro" (ID: `ZI6VUFdg6hEhnCbh`), este nodo se usa como:

- **Rol**: Motor cognitivo del AI Agent
- **Modelo**: `gemini-2.5-flash` (velocidad y eficiencia)
- **Temperature**: `0.1` (clasificaci√≥n determin√≠stica)
- **Prop√≥sito**: Analizar mensajes de usuario desde Telegram, clasificarlos sem√°nticamente, y estructurar datos para inserci√≥n en MySQL
- **Ventajas clave**:
  - Latencia < 1 segundo (experiencia instant√°nea en Telegram)
  - JSON Schema Enforcement nativo (sin validaci√≥n manual)
  - Multimodal (soporta texto, audio, im√°genes desde Telegram)
  - Razonamiento complejo sin costos excesivos

**Flujo completo**:
1. Usuario env√≠a mensaje a Telegram
2. Webhook activa workflow en n8n
3. AI Agent usa Google Gemini Chat Model para analizar
4. Gemini clasifica y estructura datos (JSON)
5. n8n inserta en MySQL con metadatos
6. Usuario recibe confirmaci√≥n en Telegram

---

**√öltima actualizaci√≥n**: 2026-01-16
**Agente**: n8n-node-documenter
**Versi√≥n del nodo**: 1
